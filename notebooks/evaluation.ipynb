{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from peft import LoraConfig, TaskType\n",
    "from peft import get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils from ../src/utils\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import get_mnli\n",
    "from utils.evaluation import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The difference between “it” aka “Instruction Tuned”\n",
    "and the base model is that the “it” variants are better for chat purposes\n",
    "since they have been fine-tuned to better understand the instructions\n",
    "and generate better answers while the base variants are those that have not undergone\n",
    "under any sort of fine-tuning. They can still generate answers but not as good as the “it” one.\n",
    "\n",
    "\"\"\"\n",
    "# google/gemma-2b | google/gemma-2b-it | microsoft/phi-2\n",
    "# Qwen/Qwen1.5-0.5B | Qwen/Qwen1.5-0.5B-Chat\n",
    "checkpoint_path = \"checkpoints/celestial-resonance-10\" \n",
    "base_model = \"microsoft/phi-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maparla/anaconda3/envs/ml/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2175f70d14964eb2995708f482a48509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: checkpoints/celestial-resonance-10\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"  #{\"\":0},\n",
    ")\n",
    "print(f\"Model loaded: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: checkpoints/celestial-resonance-10\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token # Most LLMs don't have a pad token by default\n",
    "#data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "max_seq_length = 1024\n",
    "print(f\"Tokenizer loaded: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset: MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_mnli(tokenizer, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['class_label', 'idx', 'prompt_length', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 391678\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['class_label', 'idx', 'prompt_length', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1024\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['class_label', 'idx', 'prompt_length', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['class_label', 'idx', 'prompt_length', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch dataloader format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1  # number of examples in each batch\n",
    "inference_batch_size = 1  # number of examples in each batch for inference\n",
    "max_output_tokens = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the data to tensors\n",
    "dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset size: 1000\n"
     ]
    }
   ],
   "source": [
    "val_dataloader = DataLoader(\n",
    "    dataset[\"validation\"].shuffle(seed=42).select(range(1000)),\n",
    "    batch_size=inference_batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "print(f\"Validation dataset size: {len(val_dataloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d8da8bf48e46d1aca929547b19daf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maparla/anaconda3/envs/ml/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/maparla/anaconda3/envs/ml/lib/python3.12/site-packages/transformers/generation/utils.py:1659: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.857"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds = evaluate(\n",
    "    model,\n",
    "    val_dataloader,\n",
    "    tokenizer,\n",
    "    max_output_tokens=max_output_tokens\n",
    ")\n",
    "val_correct = sum([1 for p in val_preds if p.y_true == p.y_pred])\n",
    "val_accuracy = val_correct / len(val_preds)\n",
    "val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Matched dataset size: 9815\n"
     ]
    }
   ],
   "source": [
    "test_matched_dataloader = DataLoader(\n",
    "    dataset[\"test_matched\"],\n",
    "    batch_size=inference_batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "print(f\"Test Matched dataset size: {len(test_matched_dataloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8561385634233316"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_matched_preds = evaluate(\n",
    "    model,\n",
    "    test_matched_dataloader,\n",
    "    tokenizer,\n",
    "    max_output_tokens=max_output_tokens\n",
    ")\n",
    "\n",
    "test_matched_correct = sum([1 for p in test_matched_preds if p.y_true == p.y_pred])\n",
    "test_matched_accuracy = test_matched_correct / len(test_matched_preds)\n",
    "test_matched_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test MisMatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mismatched dataset size: 9832\n"
     ]
    }
   ],
   "source": [
    "test_mismatched_dataloader = DataLoader(\n",
    "    dataset[\"test_mismatched\"],\n",
    "    batch_size=inference_batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "print(f\"Test Mismatched dataset size: {len(test_mismatched_dataloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6603320e4a6346419123a571fd92ea42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/9832 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maparla/anaconda3/envs/ml/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/maparla/anaconda3/envs/ml/lib/python3.12/site-packages/transformers/generation/utils.py:1659: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_mismatched_preds = evaluate(\n",
    "    model,\n",
    "    test_mismatched_dataloader,\n",
    "    tokenizer,\n",
    "    max_output_tokens=max_output_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8567941415785191"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mismatched_correct = sum([1 for p in test_mismatched_preds if p.y_true == p.y_pred])\n",
    "test_mismatched_accuracy = test_mismatched_correct / len(test_mismatched_preds)\n",
    "test_mismatched_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
