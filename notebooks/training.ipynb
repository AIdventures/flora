{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from peft import LoraConfig, TaskType\n",
    "from peft import get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils from ../src/utils\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import get_mnli\n",
    "from utils.evaluation import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The difference between “it” aka “Instruction Tuned”\n",
    "and the base model is that the “it” variants are better for chat purposes\n",
    "since they have been fine-tuned to better understand the instructions\n",
    "and generate better answers while the base variants are those that have not undergone\n",
    "under any sort of fine-tuning. They can still generate answers but not as good as the “it” one.\n",
    "\n",
    "\"\"\"\n",
    "# google/gemma-2b | google/gemma-2b-it | microsoft/phi-2\n",
    "# Qwen/Qwen1.5-0.5B | Qwen/Qwen1.5-0.5B-Chat\n",
    "model_name = \"microsoft/phi-2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4581e7e597cc492483be0d31e230307d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: microsoft/phi-2\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"  #{\"\":0},\n",
    ")\n",
    "print(f\"Model loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], #, \"o_proj\", \"k_proj\" | \"gate_proj\", \"up_proj\", \"down_proj\", \"dense\"],\n",
    "    #target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\", \"fc1\", \"fc2\"], #, \"o_proj\", \"k_proj\" | \"gate_proj\", \"up_proj\", \"down_proj\", \"dense\"],\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,621,440 || all params: 2,782,305,280 || trainable%: 0.0942\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: microsoft/phi-2\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token # Most LLMs don't have a pad token by default\n",
    "#data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "max_seq_length = 1024\n",
    "print(f\"Tokenizer loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset: MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_key = \"input_ids\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_mnli(tokenizer, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['class_label', 'idx', 'prompt_length', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 391678\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['class_label', 'idx', 'prompt_length', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1024\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['class_label', 'idx', 'prompt_length', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['class_label', 'idx', 'prompt_length', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch dataloader format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1  # number of examples in each batch\n",
    "inference_batch_size = 1  # number of examples in each batch for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the data to tensors\n",
    "dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 391678\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset[\"train\"],  # For testing purposes ->  .shuffle(seed=42).select(range(1000)),\n",
    "    shuffle=True, batch_size=batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset size: 1000\n"
     ]
    }
   ],
   "source": [
    "val_dataloader = DataLoader(\n",
    "    dataset[\"validation\"].shuffle(seed=42).select(range(1000)),\n",
    "    batch_size=inference_batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "print(f\"Validation dataset size: {len(val_dataloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check - Generate Batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_label': torch.Size([1]), 'idx': torch.Size([1]), 'prompt_length': torch.Size([1]), 'input_ids': torch.Size([1, 103]), 'attention_mask': torch.Size([1, 103]), 'labels': torch.Size([1, 103])}\n"
     ]
    }
   ],
   "source": [
    "print({k: v.shape for k, v in batch.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given a premise and a hypothesis below. If the premise entails the  hypothesis, return 0. If the premise contradicts the hypothesis, return 2.  Otherwise, if the premise does neither, return 1.\n",
      "\n",
      "### Premise: yeah yeah well that's neat do you do you look forward to doing it or do you sometimes have to force yourself until you get started\n",
      "\n",
      "### Hypothesis: that's neat, how often do you usually do this?\n",
      "\n",
      "### Label: \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(batch[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the base model (not finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_output_tokens = 64\n",
    "temperature = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "You are given a premise and a hypothesis below. If the premise entails the  hypothesis, return 0. If the premise contradicts the hypothesis, return 2.  Otherwise, if the premise does neither, return 1.\n",
    "\n",
    "### Premise: yeah yeah well that's neat do you do you look forward to doing it or do you sometimes have to force yourself until you get started\n",
    "\n",
    "### Hypothesis: that's neat, how often do you usually do this?\n",
    "\n",
    "### Label:\"\"\"\n",
    "#texto = tokenizer.decode(batch[prompt_key][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maparla/anaconda3/envs/ml/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/maparla/anaconda3/envs/ml/lib/python3.12/site-packages/transformers/generation/utils.py:1659: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "generated_tokens_with_prompt = model.generate(\n",
    "    tokenizer(text, return_tensors=\"pt\")[\"input_ids\"],\n",
    "    # max_length=max_input_len + max_output_tokens,\n",
    "    max_new_tokens=max_output_tokens,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    temperature=temperature,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will be trained with 40000 examples\n",
      "Total epochs: 0.10212470447663642\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0002\n",
    "weight_decay = 0.01\n",
    "\n",
    "training_steps = 40000\n",
    "training_iters = training_steps\n",
    "\n",
    "# The desired batch size is the batch size you want to train with\n",
    "effective_batch_size = 16\n",
    "gradient_accumulation_steps = effective_batch_size // batch_size\n",
    "\n",
    "\n",
    "warmup_steps = 1200  # training steps\n",
    "\n",
    "\"\"\"\n",
    "Each model is trained for 40000 training\n",
    "steps with batch size 1. Gradients are applied over\n",
    "16 accumulation steps for an effective batch size of 16.\n",
    "\"\"\"\n",
    "#training_iters = training_steps // gradient_accumulation_steps\n",
    "eval_interval = 250\n",
    "\n",
    "print(f\"The model will be trained with {training_steps*batch_size} examples\")\n",
    "#print(f\"Number of training iterations: {training_iters}\")\n",
    "print(f\"Total epochs: {training_steps*batch_size/len(dataset['train'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: celestial-resonance-10. Visit at https://wandb.ai/marioparreno/FLoRA/runs/wyob0noq\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "wandb_project = 'FLoRA'\n",
    "run = wandb.init(project=wandb_project, config={\n",
    "    \"model\": model_name,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"training_steps\": training_steps,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"effective_batch_size\": effective_batch_size,\n",
    "    \"warmup_steps\": warmup_steps,\n",
    "    \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "    \"eval_interval\": eval_interval,\n",
    "})\n",
    "print(f'Run name: {run.name}. Visit at {run.get_url()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and learning rate scheduler\n",
    "\n",
    "Create an optimizer and learning rate scheduler to fine-tune the model. Let’s use the [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) optimizer from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the default learning rate scheduler from [Trainer](https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.Trainer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\", optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps, num_training_steps=training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_keys = [\"input_ids\", \"attention_mask\", \"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb5451ab942415091209b07fcdae548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ITER 0 ###\n",
      "Train Loss: 2.7801\n",
      "### ITER 50 ###\n",
      "Train Loss: 2.6893\n",
      "### ITER 100 ###\n",
      "Train Loss: 3.1179\n",
      "### ITER 150 ###\n",
      "Train Loss: 2.1834\n",
      "### ITER 200 ###\n",
      "Train Loss: 2.0936\n",
      "### ITER 250 ###\n",
      "Train Loss: 1.0197\n",
      "### ITER 300 ###\n",
      "Train Loss: 1.0271\n",
      "### ITER 350 ###\n",
      "Train Loss: 0.9381\n",
      "### ITER 400 ###\n",
      "Train Loss: 1.1849\n",
      "### ITER 450 ###\n",
      "Train Loss: 0.8209\n",
      "### ITER 500 ###\n",
      "Train Loss: 0.5135\n",
      "### ITER 550 ###\n",
      "Train Loss: 1.8095\n",
      "### ITER 600 ###\n",
      "Train Loss: 0.8883\n",
      "### ITER 650 ###\n",
      "Train Loss: 2.1282\n",
      "### ITER 700 ###\n",
      "Train Loss: 0.9471\n",
      "### ITER 750 ###\n",
      "Train Loss: 0.4791\n",
      "### ITER 800 ###\n",
      "Train Loss: 0.7849\n",
      "### ITER 850 ###\n",
      "Train Loss: 1.5002\n",
      "### ITER 900 ###\n",
      "Train Loss: 1.6553\n",
      "### ITER 950 ###\n",
      "Train Loss: 1.1671\n",
      "### ITER 1000 ###\n",
      "Train Loss: 1.0054\n",
      "### ITER 1050 ###\n",
      "Train Loss: 1.3105\n",
      "### ITER 1100 ###\n",
      "Train Loss: 0.6235\n",
      "### ITER 1150 ###\n",
      "Train Loss: 0.4075\n",
      "### ITER 1200 ###\n",
      "Train Loss: 0.8940\n",
      "### ITER 1250 ###\n",
      "Train Loss: 0.9193\n",
      "### ITER 1300 ###\n",
      "Train Loss: 1.4406\n",
      "### ITER 1350 ###\n",
      "Train Loss: 1.6726\n",
      "### ITER 1400 ###\n",
      "Train Loss: 0.8073\n",
      "### ITER 1450 ###\n",
      "Train Loss: 1.2009\n",
      "### ITER 1500 ###\n",
      "Train Loss: 1.3270\n",
      "### ITER 1550 ###\n",
      "Train Loss: 1.1489\n",
      "### ITER 1600 ###\n",
      "Train Loss: 0.8751\n",
      "### ITER 1650 ###\n",
      "Train Loss: 0.9359\n",
      "### ITER 1700 ###\n",
      "Train Loss: 1.4913\n",
      "### ITER 1750 ###\n",
      "Train Loss: 0.8538\n",
      "### ITER 1800 ###\n",
      "Train Loss: 1.6529\n",
      "### ITER 1850 ###\n",
      "Train Loss: 1.0215\n",
      "### ITER 1900 ###\n",
      "Train Loss: 0.7526\n",
      "### ITER 1950 ###\n",
      "Train Loss: 1.1892\n",
      "### ITER 2000 ###\n",
      "Train Loss: 1.7280\n",
      "### ITER 2050 ###\n",
      "Train Loss: 1.1469\n",
      "### ITER 2100 ###\n",
      "Train Loss: 0.9218\n",
      "### ITER 2150 ###\n",
      "Train Loss: 1.3835\n",
      "### ITER 2200 ###\n",
      "Train Loss: 0.4571\n",
      "### ITER 2250 ###\n",
      "Train Loss: 1.2406\n",
      "### ITER 2300 ###\n",
      "Train Loss: 1.3140\n",
      "### ITER 2350 ###\n",
      "Train Loss: 0.5996\n",
      "### ITER 2400 ###\n",
      "Train Loss: 1.3636\n",
      "### ITER 2450 ###\n",
      "Train Loss: 1.3056\n",
      "### ITER 2500 ###\n",
      "Train Loss: 1.5859\n",
      "### ITER 2550 ###\n",
      "Train Loss: 2.0511\n",
      "### ITER 2600 ###\n",
      "Train Loss: 0.8820\n",
      "### ITER 2650 ###\n",
      "Train Loss: 1.0737\n",
      "### ITER 2700 ###\n",
      "Train Loss: 1.5627\n",
      "### ITER 2750 ###\n",
      "Train Loss: 1.3174\n",
      "### ITER 2800 ###\n",
      "Train Loss: 0.7682\n",
      "### ITER 2850 ###\n",
      "Train Loss: 0.9907\n",
      "### ITER 2900 ###\n",
      "Train Loss: 1.7759\n",
      "### ITER 2950 ###\n",
      "Train Loss: 1.8607\n",
      "### ITER 3000 ###\n",
      "Train Loss: 1.1722\n",
      "### ITER 3050 ###\n",
      "Train Loss: 1.7421\n",
      "### ITER 3100 ###\n",
      "Train Loss: 1.0280\n",
      "### ITER 3150 ###\n",
      "Train Loss: 1.0401\n",
      "### ITER 3200 ###\n",
      "Train Loss: 1.1414\n",
      "### ITER 3250 ###\n",
      "Train Loss: 0.2849\n",
      "### ITER 3300 ###\n",
      "Train Loss: 0.7707\n",
      "### ITER 3350 ###\n",
      "Train Loss: 1.5167\n",
      "### ITER 3400 ###\n",
      "Train Loss: 1.0625\n",
      "### ITER 3450 ###\n",
      "Train Loss: 1.0397\n",
      "### ITER 3500 ###\n",
      "Train Loss: 2.0043\n",
      "### ITER 3550 ###\n",
      "Train Loss: 1.6536\n",
      "### ITER 3600 ###\n",
      "Train Loss: 1.1197\n",
      "### ITER 3650 ###\n",
      "Train Loss: 1.2200\n",
      "### ITER 3700 ###\n",
      "Train Loss: 1.5697\n",
      "### ITER 3750 ###\n",
      "Train Loss: 1.3575\n",
      "### ITER 3800 ###\n",
      "Train Loss: 1.0622\n",
      "### ITER 3850 ###\n",
      "Train Loss: 1.1230\n",
      "### ITER 3900 ###\n",
      "Train Loss: 1.5709\n",
      "### ITER 3950 ###\n",
      "Train Loss: 1.7320\n",
      "### ITER 4000 ###\n",
      "Train Loss: 0.6821\n",
      "### ITER 4050 ###\n",
      "Train Loss: 1.2192\n",
      "### ITER 4100 ###\n",
      "Train Loss: 1.2366\n",
      "### ITER 4150 ###\n",
      "Train Loss: 1.0236\n",
      "### ITER 4200 ###\n",
      "Train Loss: 0.5711\n",
      "### ITER 4250 ###\n",
      "Train Loss: 0.5479\n",
      "### ITER 4300 ###\n",
      "Train Loss: 0.8089\n",
      "### ITER 4350 ###\n",
      "Train Loss: 0.6477\n",
      "### ITER 4400 ###\n",
      "Train Loss: 1.2467\n",
      "### ITER 4450 ###\n",
      "Train Loss: 1.8106\n",
      "### ITER 4500 ###\n",
      "Train Loss: 1.3686\n",
      "### ITER 4550 ###\n",
      "Train Loss: 1.1319\n",
      "### ITER 4600 ###\n",
      "Train Loss: 0.9029\n",
      "### ITER 4650 ###\n",
      "Train Loss: 1.0588\n",
      "### ITER 4700 ###\n",
      "Train Loss: 0.6377\n",
      "### ITER 4750 ###\n",
      "Train Loss: 0.8670\n",
      "### ITER 4800 ###\n",
      "Train Loss: 0.8169\n",
      "### ITER 4850 ###\n",
      "Train Loss: 1.0811\n",
      "### ITER 4900 ###\n",
      "Train Loss: 1.4337\n",
      "### ITER 4950 ###\n",
      "Train Loss: 1.2827\n",
      "### ITER 5000 ###\n",
      "Train Loss: 1.0711\n",
      "### ITER 5050 ###\n",
      "Train Loss: 1.0719\n",
      "### ITER 5100 ###\n",
      "Train Loss: 0.8949\n",
      "### ITER 5150 ###\n",
      "Train Loss: 0.9778\n",
      "### ITER 5200 ###\n",
      "Train Loss: 0.7304\n",
      "### ITER 5250 ###\n",
      "Train Loss: 0.8514\n",
      "### ITER 5300 ###\n",
      "Train Loss: 1.0114\n",
      "### ITER 5350 ###\n",
      "Train Loss: 1.3827\n",
      "### ITER 5400 ###\n",
      "Train Loss: 0.8873\n",
      "### ITER 5450 ###\n",
      "Train Loss: 0.9078\n",
      "### ITER 5500 ###\n",
      "Train Loss: 1.0795\n",
      "### ITER 5550 ###\n",
      "Train Loss: 0.7183\n",
      "### ITER 5600 ###\n",
      "Train Loss: 1.3146\n",
      "### ITER 5650 ###\n",
      "Train Loss: 0.3224\n",
      "### ITER 5700 ###\n",
      "Train Loss: 1.0849\n",
      "### ITER 5750 ###\n",
      "Train Loss: 1.0449\n",
      "### ITER 5800 ###\n",
      "Train Loss: 0.6820\n",
      "### ITER 5850 ###\n",
      "Train Loss: 1.0804\n",
      "### ITER 5900 ###\n",
      "Train Loss: 0.6209\n",
      "### ITER 5950 ###\n",
      "Train Loss: 1.1445\n",
      "### ITER 6000 ###\n",
      "Train Loss: 0.8988\n",
      "### ITER 6050 ###\n",
      "Train Loss: 1.3941\n",
      "### ITER 6100 ###\n",
      "Train Loss: 1.3584\n",
      "### ITER 6150 ###\n",
      "Train Loss: 0.8254\n",
      "### ITER 6200 ###\n",
      "Train Loss: 1.1869\n",
      "### ITER 6250 ###\n",
      "Train Loss: 1.2985\n",
      "### ITER 6300 ###\n",
      "Train Loss: 1.4196\n",
      "### ITER 6350 ###\n",
      "Train Loss: 0.8626\n",
      "### ITER 6400 ###\n",
      "Train Loss: 1.6778\n",
      "### ITER 6450 ###\n",
      "Train Loss: 1.3501\n",
      "### ITER 6500 ###\n",
      "Train Loss: 1.1768\n",
      "### ITER 6550 ###\n",
      "Train Loss: 1.2099\n",
      "### ITER 6600 ###\n",
      "Train Loss: 0.9862\n",
      "### ITER 6650 ###\n",
      "Train Loss: 1.1677\n",
      "### ITER 6700 ###\n",
      "Train Loss: 1.5604\n",
      "### ITER 6750 ###\n",
      "Train Loss: 0.6926\n",
      "### ITER 6800 ###\n",
      "Train Loss: 0.9077\n",
      "### ITER 6850 ###\n",
      "Train Loss: 0.9545\n",
      "### ITER 6900 ###\n",
      "Train Loss: 0.8915\n",
      "### ITER 6950 ###\n",
      "Train Loss: 1.4195\n",
      "### ITER 7000 ###\n",
      "Train Loss: 0.8653\n",
      "### ITER 7050 ###\n",
      "Train Loss: 0.9288\n",
      "### ITER 7100 ###\n",
      "Train Loss: 1.3720\n",
      "### ITER 7150 ###\n",
      "Train Loss: 1.0780\n",
      "### ITER 7200 ###\n",
      "Train Loss: 0.9221\n",
      "### ITER 7250 ###\n",
      "Train Loss: 1.3526\n",
      "### ITER 7300 ###\n",
      "Train Loss: 1.7106\n",
      "### ITER 7350 ###\n",
      "Train Loss: 0.3374\n",
      "### ITER 7400 ###\n",
      "Train Loss: 0.8954\n",
      "### ITER 7450 ###\n",
      "Train Loss: 1.0190\n",
      "### ITER 7500 ###\n",
      "Train Loss: 1.4513\n",
      "### ITER 7550 ###\n",
      "Train Loss: 2.3248\n",
      "### ITER 7600 ###\n",
      "Train Loss: 0.9737\n",
      "### ITER 7650 ###\n",
      "Train Loss: 0.9944\n",
      "### ITER 7700 ###\n",
      "Train Loss: 0.8126\n",
      "### ITER 7750 ###\n",
      "Train Loss: 1.5102\n",
      "### ITER 7800 ###\n",
      "Train Loss: 1.3246\n",
      "### ITER 7850 ###\n",
      "Train Loss: 0.9201\n",
      "### ITER 7900 ###\n",
      "Train Loss: 1.1760\n",
      "### ITER 7950 ###\n",
      "Train Loss: 1.2429\n",
      "### ITER 8000 ###\n",
      "Train Loss: 1.0062\n",
      "### ITER 8050 ###\n",
      "Train Loss: 1.4451\n",
      "### ITER 8100 ###\n",
      "Train Loss: 1.2593\n",
      "### ITER 8150 ###\n",
      "Train Loss: 1.6345\n",
      "### ITER 8200 ###\n",
      "Train Loss: 1.4017\n",
      "### ITER 8250 ###\n",
      "Train Loss: 1.4093\n",
      "### ITER 8300 ###\n",
      "Train Loss: 1.0286\n",
      "### ITER 8350 ###\n",
      "Train Loss: 1.0960\n",
      "### ITER 8400 ###\n",
      "Train Loss: 1.4353\n",
      "### ITER 8450 ###\n",
      "Train Loss: 1.0087\n",
      "### ITER 8500 ###\n",
      "Train Loss: 0.7947\n",
      "### ITER 8550 ###\n",
      "Train Loss: 1.0704\n",
      "### ITER 8600 ###\n",
      "Train Loss: 0.9714\n",
      "### ITER 8650 ###\n",
      "Train Loss: 1.0756\n",
      "### ITER 8700 ###\n",
      "Train Loss: 0.4051\n",
      "### ITER 8750 ###\n",
      "Train Loss: 0.7416\n",
      "### ITER 8800 ###\n",
      "Train Loss: 0.9559\n",
      "### ITER 8850 ###\n",
      "Train Loss: 1.0679\n",
      "### ITER 8900 ###\n",
      "Train Loss: 0.2531\n",
      "### ITER 8950 ###\n",
      "Train Loss: 1.0991\n",
      "### ITER 9000 ###\n",
      "Train Loss: 0.7841\n",
      "### ITER 9050 ###\n",
      "Train Loss: 1.1152\n",
      "### ITER 9100 ###\n",
      "Train Loss: 0.8182\n",
      "### ITER 9150 ###\n",
      "Train Loss: 0.7537\n",
      "### ITER 9200 ###\n",
      "Train Loss: 1.4546\n",
      "### ITER 9250 ###\n",
      "Train Loss: 0.5634\n",
      "### ITER 9300 ###\n",
      "Train Loss: 0.7812\n",
      "### ITER 9350 ###\n",
      "Train Loss: 1.1799\n",
      "### ITER 9400 ###\n",
      "Train Loss: 1.4279\n",
      "### ITER 9450 ###\n",
      "Train Loss: 0.9520\n",
      "### ITER 9500 ###\n",
      "Train Loss: 1.4066\n",
      "### ITER 9550 ###\n",
      "Train Loss: 0.5709\n",
      "### ITER 9600 ###\n",
      "Train Loss: 1.0099\n",
      "### ITER 9650 ###\n",
      "Train Loss: 1.0128\n",
      "### ITER 9700 ###\n",
      "Train Loss: 1.1747\n",
      "### ITER 9750 ###\n",
      "Train Loss: 1.5782\n",
      "### ITER 9800 ###\n",
      "Train Loss: 1.2742\n",
      "### ITER 9850 ###\n",
      "Train Loss: 0.8507\n",
      "### ITER 9900 ###\n",
      "Train Loss: 1.1297\n",
      "### ITER 9950 ###\n",
      "Train Loss: 0.5694\n",
      "### ITER 10000 ###\n",
      "Train Loss: 1.5547\n",
      "### ITER 10050 ###\n",
      "Train Loss: 1.3288\n",
      "### ITER 10100 ###\n",
      "Train Loss: 1.4219\n",
      "### ITER 10150 ###\n",
      "Train Loss: 0.8402\n",
      "### ITER 10200 ###\n",
      "Train Loss: 1.0145\n",
      "### ITER 10250 ###\n",
      "Train Loss: 1.1352\n",
      "### ITER 10300 ###\n",
      "Train Loss: 1.2417\n",
      "### ITER 10350 ###\n",
      "Train Loss: 1.1002\n",
      "### ITER 10400 ###\n",
      "Train Loss: 0.8343\n",
      "### ITER 10450 ###\n",
      "Train Loss: 2.1284\n",
      "### ITER 10500 ###\n",
      "Train Loss: 1.0169\n",
      "### ITER 10550 ###\n",
      "Train Loss: 1.4659\n",
      "### ITER 10600 ###\n",
      "Train Loss: 0.4813\n",
      "### ITER 10650 ###\n",
      "Train Loss: 0.8908\n",
      "### ITER 10700 ###\n",
      "Train Loss: 1.0599\n",
      "### ITER 10750 ###\n",
      "Train Loss: 0.7542\n",
      "### ITER 10800 ###\n",
      "Train Loss: 1.1799\n",
      "### ITER 10850 ###\n",
      "Train Loss: 1.5105\n",
      "### ITER 10900 ###\n",
      "Train Loss: 0.9208\n",
      "### ITER 10950 ###\n",
      "Train Loss: 1.0986\n",
      "### ITER 11000 ###\n",
      "Train Loss: 0.9914\n",
      "### ITER 11050 ###\n",
      "Train Loss: 1.1335\n",
      "### ITER 11100 ###\n",
      "Train Loss: 0.8041\n",
      "### ITER 11150 ###\n",
      "Train Loss: 1.0796\n",
      "### ITER 11200 ###\n",
      "Train Loss: 1.3619\n",
      "### ITER 11250 ###\n",
      "Train Loss: 1.2162\n",
      "### ITER 11300 ###\n",
      "Train Loss: 1.3965\n",
      "### ITER 11350 ###\n",
      "Train Loss: 1.4636\n",
      "### ITER 11400 ###\n",
      "Train Loss: 1.0596\n",
      "### ITER 11450 ###\n",
      "Train Loss: 1.3783\n",
      "### ITER 11500 ###\n",
      "Train Loss: 0.9821\n",
      "### ITER 11550 ###\n",
      "Train Loss: 1.0009\n",
      "### ITER 11600 ###\n",
      "Train Loss: 1.2314\n",
      "### ITER 11650 ###\n",
      "Train Loss: 0.8435\n",
      "### ITER 11700 ###\n",
      "Train Loss: 1.0908\n",
      "### ITER 11750 ###\n",
      "Train Loss: 1.6589\n",
      "### ITER 11800 ###\n",
      "Train Loss: 1.1461\n",
      "### ITER 11850 ###\n",
      "Train Loss: 0.9049\n",
      "### ITER 11900 ###\n",
      "Train Loss: 1.2760\n",
      "### ITER 11950 ###\n",
      "Train Loss: 1.1259\n",
      "### ITER 12000 ###\n",
      "Train Loss: 0.8159\n",
      "### ITER 12050 ###\n",
      "Train Loss: 1.6832\n",
      "### ITER 12100 ###\n",
      "Train Loss: 1.3389\n",
      "### ITER 12150 ###\n",
      "Train Loss: 0.8281\n",
      "### ITER 12200 ###\n",
      "Train Loss: 1.3913\n",
      "### ITER 12250 ###\n",
      "Train Loss: 1.6966\n",
      "### ITER 12300 ###\n",
      "Train Loss: 0.9020\n",
      "### ITER 12350 ###\n",
      "Train Loss: 0.6572\n",
      "### ITER 12400 ###\n",
      "Train Loss: 1.0884\n",
      "### ITER 12450 ###\n",
      "Train Loss: 1.8225\n",
      "### ITER 12500 ###\n",
      "Train Loss: 0.8424\n",
      "### ITER 12550 ###\n",
      "Train Loss: 0.8679\n",
      "### ITER 12600 ###\n",
      "Train Loss: 1.2373\n",
      "### ITER 12650 ###\n",
      "Train Loss: 1.0892\n",
      "### ITER 12700 ###\n",
      "Train Loss: 1.2528\n",
      "### ITER 12750 ###\n",
      "Train Loss: 0.7990\n",
      "### ITER 12800 ###\n",
      "Train Loss: 0.6834\n",
      "### ITER 12850 ###\n",
      "Train Loss: 1.0441\n",
      "### ITER 12900 ###\n",
      "Train Loss: 1.4590\n",
      "### ITER 12950 ###\n",
      "Train Loss: 1.2616\n",
      "### ITER 13000 ###\n",
      "Train Loss: 0.9885\n",
      "### ITER 13050 ###\n",
      "Train Loss: 0.7859\n",
      "### ITER 13100 ###\n",
      "Train Loss: 1.2588\n",
      "### ITER 13150 ###\n",
      "Train Loss: 0.6508\n",
      "### ITER 13200 ###\n",
      "Train Loss: 1.2701\n",
      "### ITER 13250 ###\n",
      "Train Loss: 1.3872\n",
      "### ITER 13300 ###\n",
      "Train Loss: 0.7878\n",
      "### ITER 13350 ###\n",
      "Train Loss: 1.1788\n",
      "### ITER 13400 ###\n",
      "Train Loss: 1.0272\n",
      "### ITER 13450 ###\n",
      "Train Loss: 1.0028\n",
      "### ITER 13500 ###\n",
      "Train Loss: 1.0647\n",
      "### ITER 13550 ###\n",
      "Train Loss: 0.6981\n",
      "### ITER 13600 ###\n",
      "Train Loss: 1.8086\n",
      "### ITER 13650 ###\n",
      "Train Loss: 1.8377\n",
      "### ITER 13700 ###\n",
      "Train Loss: 1.1871\n",
      "### ITER 13750 ###\n",
      "Train Loss: 0.9540\n",
      "### ITER 13800 ###\n",
      "Train Loss: 1.0817\n",
      "### ITER 13850 ###\n",
      "Train Loss: 1.2245\n",
      "### ITER 13900 ###\n",
      "Train Loss: 1.1899\n",
      "### ITER 13950 ###\n",
      "Train Loss: 1.2846\n",
      "### ITER 14000 ###\n",
      "Train Loss: 0.6896\n",
      "### ITER 14050 ###\n",
      "Train Loss: 1.7019\n",
      "### ITER 14100 ###\n",
      "Train Loss: 1.6141\n",
      "### ITER 14150 ###\n",
      "Train Loss: 1.1661\n",
      "### ITER 14200 ###\n",
      "Train Loss: 1.0001\n",
      "### ITER 14250 ###\n",
      "Train Loss: 0.9971\n",
      "### ITER 14300 ###\n",
      "Train Loss: 1.6839\n",
      "### ITER 14350 ###\n",
      "Train Loss: 1.0200\n",
      "### ITER 14400 ###\n",
      "Train Loss: 0.9831\n",
      "### ITER 14450 ###\n",
      "Train Loss: 0.7088\n",
      "### ITER 14500 ###\n",
      "Train Loss: 2.0043\n",
      "### ITER 14550 ###\n",
      "Train Loss: 1.0703\n",
      "### ITER 14600 ###\n",
      "Train Loss: 0.5090\n",
      "### ITER 14650 ###\n",
      "Train Loss: 1.2376\n",
      "### ITER 14700 ###\n",
      "Train Loss: 0.9589\n",
      "### ITER 14750 ###\n",
      "Train Loss: 1.0651\n",
      "### ITER 14800 ###\n",
      "Train Loss: 0.9209\n",
      "### ITER 14850 ###\n",
      "Train Loss: 0.8213\n",
      "### ITER 14900 ###\n",
      "Train Loss: 1.2329\n",
      "### ITER 14950 ###\n",
      "Train Loss: 1.0131\n",
      "### ITER 15000 ###\n",
      "Train Loss: 1.6265\n",
      "### ITER 15050 ###\n",
      "Train Loss: 0.7752\n",
      "### ITER 15100 ###\n",
      "Train Loss: 1.4048\n",
      "### ITER 15150 ###\n",
      "Train Loss: 1.3284\n",
      "### ITER 15200 ###\n",
      "Train Loss: 0.5652\n",
      "### ITER 15250 ###\n",
      "Train Loss: 0.8892\n",
      "### ITER 15300 ###\n",
      "Train Loss: 1.5177\n",
      "### ITER 15350 ###\n",
      "Train Loss: 0.9252\n",
      "### ITER 15400 ###\n",
      "Train Loss: 0.6890\n",
      "### ITER 15450 ###\n",
      "Train Loss: 0.6625\n",
      "### ITER 15500 ###\n",
      "Train Loss: 0.9989\n",
      "### ITER 15550 ###\n",
      "Train Loss: 1.2298\n",
      "### ITER 15600 ###\n",
      "Train Loss: 1.3111\n",
      "### ITER 15650 ###\n",
      "Train Loss: 2.2734\n",
      "### ITER 15700 ###\n",
      "Train Loss: 0.6905\n",
      "### ITER 15750 ###\n",
      "Train Loss: 0.8641\n",
      "### ITER 15800 ###\n",
      "Train Loss: 0.8334\n",
      "### ITER 15850 ###\n",
      "Train Loss: 0.8855\n",
      "### ITER 15900 ###\n",
      "Train Loss: 0.9967\n",
      "### ITER 15950 ###\n",
      "Train Loss: 0.6597\n",
      "### ITER 16000 ###\n",
      "Train Loss: 0.5945\n",
      "### ITER 16050 ###\n",
      "Train Loss: 1.5161\n",
      "### ITER 16100 ###\n",
      "Train Loss: 1.1294\n",
      "### ITER 16150 ###\n",
      "Train Loss: 0.8452\n",
      "### ITER 16200 ###\n",
      "Train Loss: 0.4142\n",
      "### ITER 16250 ###\n",
      "Train Loss: 0.7343\n",
      "### ITER 16300 ###\n",
      "Train Loss: 0.9435\n",
      "### ITER 16350 ###\n",
      "Train Loss: 1.0900\n",
      "### ITER 16400 ###\n",
      "Train Loss: 1.4796\n",
      "### ITER 16450 ###\n",
      "Train Loss: 1.3627\n",
      "### ITER 16500 ###\n",
      "Train Loss: 0.8705\n",
      "### ITER 16550 ###\n",
      "Train Loss: 0.5037\n",
      "### ITER 16600 ###\n",
      "Train Loss: 0.2744\n",
      "### ITER 16650 ###\n",
      "Train Loss: 1.1770\n",
      "### ITER 16700 ###\n",
      "Train Loss: 0.5968\n",
      "### ITER 16750 ###\n",
      "Train Loss: 1.3102\n",
      "### ITER 16800 ###\n",
      "Train Loss: 1.2435\n",
      "### ITER 16850 ###\n",
      "Train Loss: 1.1086\n",
      "### ITER 16900 ###\n",
      "Train Loss: 1.4326\n",
      "### ITER 16950 ###\n",
      "Train Loss: 1.1531\n",
      "### ITER 17000 ###\n",
      "Train Loss: 0.9890\n",
      "### ITER 17050 ###\n",
      "Train Loss: 1.5300\n",
      "### ITER 17100 ###\n",
      "Train Loss: 0.8947\n",
      "### ITER 17150 ###\n",
      "Train Loss: 0.7584\n",
      "### ITER 17200 ###\n",
      "Train Loss: 1.4722\n",
      "### ITER 17250 ###\n",
      "Train Loss: 0.8138\n",
      "### ITER 17300 ###\n",
      "Train Loss: 0.7410\n",
      "### ITER 17350 ###\n",
      "Train Loss: 1.4956\n",
      "### ITER 17400 ###\n",
      "Train Loss: 1.5684\n",
      "### ITER 17450 ###\n",
      "Train Loss: 0.9657\n",
      "### ITER 17500 ###\n",
      "Train Loss: 0.6005\n",
      "### ITER 17550 ###\n",
      "Train Loss: 1.1172\n",
      "### ITER 17600 ###\n",
      "Train Loss: 0.9334\n",
      "### ITER 17650 ###\n",
      "Train Loss: 0.9762\n",
      "### ITER 17700 ###\n",
      "Train Loss: 1.2692\n",
      "### ITER 17750 ###\n",
      "Train Loss: 1.0967\n",
      "### ITER 17800 ###\n",
      "Train Loss: 1.3132\n",
      "### ITER 17850 ###\n",
      "Train Loss: 1.0552\n",
      "### ITER 17900 ###\n",
      "Train Loss: 1.1837\n",
      "### ITER 17950 ###\n",
      "Train Loss: 0.5027\n",
      "### ITER 18000 ###\n",
      "Train Loss: 1.1121\n",
      "### ITER 18050 ###\n",
      "Train Loss: 0.7421\n",
      "### ITER 18100 ###\n",
      "Train Loss: 0.8551\n",
      "### ITER 18150 ###\n",
      "Train Loss: 1.2209\n",
      "### ITER 18200 ###\n",
      "Train Loss: 1.0568\n",
      "### ITER 18250 ###\n",
      "Train Loss: 1.8196\n",
      "### ITER 18300 ###\n",
      "Train Loss: 0.8864\n",
      "### ITER 18350 ###\n",
      "Train Loss: 0.9595\n",
      "### ITER 18400 ###\n",
      "Train Loss: 1.6767\n",
      "### ITER 18450 ###\n",
      "Train Loss: 1.0926\n",
      "### ITER 18500 ###\n",
      "Train Loss: 0.8715\n",
      "### ITER 18550 ###\n",
      "Train Loss: 1.7174\n",
      "### ITER 18600 ###\n",
      "Train Loss: 0.7161\n",
      "### ITER 18650 ###\n",
      "Train Loss: 1.1975\n",
      "### ITER 18700 ###\n",
      "Train Loss: 0.6372\n",
      "### ITER 18750 ###\n",
      "Train Loss: 1.0940\n",
      "### ITER 18800 ###\n",
      "Train Loss: 0.5481\n",
      "### ITER 18850 ###\n",
      "Train Loss: 1.1314\n",
      "### ITER 18900 ###\n",
      "Train Loss: 0.9937\n",
      "### ITER 18950 ###\n",
      "Train Loss: 0.7669\n",
      "### ITER 19000 ###\n",
      "Train Loss: 1.0976\n",
      "### ITER 19050 ###\n",
      "Train Loss: 0.6822\n",
      "### ITER 19100 ###\n",
      "Train Loss: 1.7943\n",
      "### ITER 19150 ###\n",
      "Train Loss: 1.1381\n",
      "### ITER 19200 ###\n",
      "Train Loss: 1.2624\n",
      "### ITER 19250 ###\n",
      "Train Loss: 0.6977\n",
      "### ITER 19300 ###\n",
      "Train Loss: 1.0100\n",
      "### ITER 19350 ###\n",
      "Train Loss: 0.5578\n",
      "### ITER 19400 ###\n",
      "Train Loss: 0.7361\n",
      "### ITER 19450 ###\n",
      "Train Loss: 0.5272\n",
      "### ITER 19500 ###\n",
      "Train Loss: 0.9210\n",
      "### ITER 19550 ###\n",
      "Train Loss: 0.7985\n",
      "### ITER 19600 ###\n",
      "Train Loss: 1.2755\n",
      "### ITER 19650 ###\n",
      "Train Loss: 1.2768\n",
      "### ITER 19700 ###\n",
      "Train Loss: 1.0487\n",
      "### ITER 19750 ###\n",
      "Train Loss: 1.0581\n",
      "### ITER 19800 ###\n",
      "Train Loss: 0.9643\n",
      "### ITER 19850 ###\n",
      "Train Loss: 1.3898\n",
      "### ITER 19900 ###\n",
      "Train Loss: 0.9065\n",
      "### ITER 19950 ###\n",
      "Train Loss: 1.4353\n",
      "### ITER 20000 ###\n",
      "Train Loss: 0.8679\n",
      "### ITER 20050 ###\n",
      "Train Loss: 0.3547\n",
      "### ITER 20100 ###\n",
      "Train Loss: 1.2166\n",
      "### ITER 20150 ###\n",
      "Train Loss: 0.8138\n",
      "### ITER 20200 ###\n",
      "Train Loss: 1.4084\n",
      "### ITER 20250 ###\n",
      "Train Loss: 1.6044\n",
      "### ITER 20300 ###\n",
      "Train Loss: 0.6436\n",
      "### ITER 20350 ###\n",
      "Train Loss: 0.6985\n",
      "### ITER 20400 ###\n",
      "Train Loss: 0.9807\n",
      "### ITER 20450 ###\n",
      "Train Loss: 0.6121\n",
      "### ITER 20500 ###\n",
      "Train Loss: 0.4987\n",
      "### ITER 20550 ###\n",
      "Train Loss: 1.9512\n",
      "### ITER 20600 ###\n",
      "Train Loss: 0.8944\n",
      "### ITER 20650 ###\n",
      "Train Loss: 0.9579\n",
      "### ITER 20700 ###\n",
      "Train Loss: 0.6457\n",
      "### ITER 20750 ###\n",
      "Train Loss: 0.7846\n",
      "### ITER 20800 ###\n",
      "Train Loss: 0.6549\n",
      "### ITER 20850 ###\n",
      "Train Loss: 1.2209\n",
      "### ITER 20900 ###\n",
      "Train Loss: 1.4872\n",
      "### ITER 20950 ###\n",
      "Train Loss: 1.5129\n",
      "### ITER 21000 ###\n",
      "Train Loss: 1.0172\n",
      "### ITER 21050 ###\n",
      "Train Loss: 1.7609\n",
      "### ITER 21100 ###\n",
      "Train Loss: 0.5885\n",
      "### ITER 21150 ###\n",
      "Train Loss: 1.3435\n",
      "### ITER 21200 ###\n",
      "Train Loss: 1.2435\n",
      "### ITER 21250 ###\n",
      "Train Loss: 1.6155\n",
      "### ITER 21300 ###\n",
      "Train Loss: 0.9216\n",
      "### ITER 21350 ###\n",
      "Train Loss: 1.5800\n",
      "### ITER 21400 ###\n",
      "Train Loss: 0.9821\n",
      "### ITER 21450 ###\n",
      "Train Loss: 0.8358\n",
      "### ITER 21500 ###\n",
      "Train Loss: 0.7533\n",
      "### ITER 21550 ###\n",
      "Train Loss: 1.3579\n",
      "### ITER 21600 ###\n",
      "Train Loss: 0.8216\n",
      "### ITER 21650 ###\n",
      "Train Loss: 1.3878\n",
      "### ITER 21700 ###\n",
      "Train Loss: 0.6787\n",
      "### ITER 21750 ###\n",
      "Train Loss: 1.2004\n",
      "### ITER 21800 ###\n",
      "Train Loss: 0.9677\n",
      "### ITER 21850 ###\n",
      "Train Loss: 0.9821\n",
      "### ITER 21900 ###\n",
      "Train Loss: 1.5808\n",
      "### ITER 21950 ###\n",
      "Train Loss: 0.6023\n",
      "### ITER 22000 ###\n",
      "Train Loss: 2.1220\n",
      "### ITER 22050 ###\n",
      "Train Loss: 0.7176\n",
      "### ITER 22100 ###\n",
      "Train Loss: 1.1553\n",
      "### ITER 22150 ###\n",
      "Train Loss: 1.0243\n",
      "### ITER 22200 ###\n",
      "Train Loss: 0.6513\n",
      "### ITER 22250 ###\n",
      "Train Loss: 0.9409\n",
      "### ITER 22300 ###\n",
      "Train Loss: 1.4524\n",
      "### ITER 22350 ###\n",
      "Train Loss: 0.7697\n",
      "### ITER 22400 ###\n",
      "Train Loss: 1.4803\n",
      "### ITER 22450 ###\n",
      "Train Loss: 0.9041\n",
      "### ITER 22500 ###\n",
      "Train Loss: 1.6218\n",
      "### ITER 22550 ###\n",
      "Train Loss: 1.4369\n",
      "### ITER 22600 ###\n",
      "Train Loss: 1.1925\n",
      "### ITER 22650 ###\n",
      "Train Loss: 0.9196\n",
      "### ITER 22700 ###\n",
      "Train Loss: 1.1492\n",
      "### ITER 22750 ###\n",
      "Train Loss: 0.8412\n",
      "### ITER 22800 ###\n",
      "Train Loss: 1.3043\n",
      "### ITER 22850 ###\n",
      "Train Loss: 1.4728\n",
      "### ITER 22900 ###\n",
      "Train Loss: 1.6826\n",
      "### ITER 22950 ###\n",
      "Train Loss: 1.0860\n",
      "### ITER 23000 ###\n",
      "Train Loss: 1.3441\n",
      "### ITER 23050 ###\n",
      "Train Loss: 1.4608\n",
      "### ITER 23100 ###\n",
      "Train Loss: 2.0594\n",
      "### ITER 23150 ###\n",
      "Train Loss: 0.9856\n",
      "### ITER 23200 ###\n",
      "Train Loss: 1.2125\n",
      "### ITER 23250 ###\n",
      "Train Loss: 0.6541\n",
      "### ITER 23300 ###\n",
      "Train Loss: 1.3246\n",
      "### ITER 23350 ###\n",
      "Train Loss: 1.1877\n",
      "### ITER 23400 ###\n",
      "Train Loss: 0.8540\n",
      "### ITER 23450 ###\n",
      "Train Loss: 1.4591\n",
      "### ITER 23500 ###\n",
      "Train Loss: 1.8082\n",
      "### ITER 23550 ###\n",
      "Train Loss: 1.2335\n",
      "### ITER 23600 ###\n",
      "Train Loss: 1.5285\n",
      "### ITER 23650 ###\n",
      "Train Loss: 1.2591\n",
      "### ITER 23700 ###\n",
      "Train Loss: 1.2436\n",
      "### ITER 23750 ###\n",
      "Train Loss: 1.1666\n",
      "### ITER 23800 ###\n",
      "Train Loss: 1.1220\n",
      "### ITER 23850 ###\n",
      "Train Loss: 1.3107\n",
      "### ITER 23900 ###\n",
      "Train Loss: 0.9733\n",
      "### ITER 23950 ###\n",
      "Train Loss: 1.1132\n",
      "### ITER 24000 ###\n",
      "Train Loss: 0.9784\n",
      "### ITER 24050 ###\n",
      "Train Loss: 1.0064\n",
      "### ITER 24100 ###\n",
      "Train Loss: 1.2863\n",
      "### ITER 24150 ###\n",
      "Train Loss: 1.6284\n",
      "### ITER 24200 ###\n",
      "Train Loss: 1.1405\n",
      "### ITER 24250 ###\n",
      "Train Loss: 0.7133\n",
      "### ITER 24300 ###\n",
      "Train Loss: 1.0712\n",
      "### ITER 24350 ###\n",
      "Train Loss: 2.5252\n",
      "### ITER 24400 ###\n",
      "Train Loss: 0.8663\n",
      "### ITER 24450 ###\n",
      "Train Loss: 1.3119\n",
      "### ITER 24500 ###\n",
      "Train Loss: 1.3971\n",
      "### ITER 24550 ###\n",
      "Train Loss: 1.1821\n",
      "### ITER 24600 ###\n",
      "Train Loss: 1.0968\n",
      "### ITER 24650 ###\n",
      "Train Loss: 0.5188\n",
      "### ITER 24700 ###\n",
      "Train Loss: 1.1181\n",
      "### ITER 24750 ###\n",
      "Train Loss: 0.8317\n",
      "### ITER 24800 ###\n",
      "Train Loss: 0.9486\n",
      "### ITER 24850 ###\n",
      "Train Loss: 1.1429\n",
      "### ITER 24900 ###\n",
      "Train Loss: 1.2683\n",
      "### ITER 24950 ###\n",
      "Train Loss: 0.5545\n",
      "### ITER 25000 ###\n",
      "Train Loss: 1.2862\n",
      "### ITER 25050 ###\n",
      "Train Loss: 0.5254\n",
      "### ITER 25100 ###\n",
      "Train Loss: 1.5144\n",
      "### ITER 25150 ###\n",
      "Train Loss: 0.9672\n",
      "### ITER 25200 ###\n",
      "Train Loss: 1.1804\n",
      "### ITER 25250 ###\n",
      "Train Loss: 1.6428\n",
      "### ITER 25300 ###\n",
      "Train Loss: 1.2700\n",
      "### ITER 25350 ###\n",
      "Train Loss: 0.5345\n",
      "### ITER 25400 ###\n",
      "Train Loss: 0.8839\n",
      "### ITER 25450 ###\n",
      "Train Loss: 0.3191\n",
      "### ITER 25500 ###\n",
      "Train Loss: 0.6822\n",
      "### ITER 25550 ###\n",
      "Train Loss: 1.3601\n",
      "### ITER 25600 ###\n",
      "Train Loss: 0.8062\n",
      "### ITER 25650 ###\n",
      "Train Loss: 0.6344\n",
      "### ITER 25700 ###\n",
      "Train Loss: 1.0052\n",
      "### ITER 25750 ###\n",
      "Train Loss: 0.4348\n",
      "### ITER 25800 ###\n",
      "Train Loss: 1.1144\n",
      "### ITER 25850 ###\n",
      "Train Loss: 0.9204\n",
      "### ITER 25900 ###\n",
      "Train Loss: 1.2678\n",
      "### ITER 25950 ###\n",
      "Train Loss: 0.9452\n",
      "### ITER 26000 ###\n",
      "Train Loss: 1.0130\n",
      "### ITER 26050 ###\n",
      "Train Loss: 1.5583\n",
      "### ITER 26100 ###\n",
      "Train Loss: 0.7175\n",
      "### ITER 26150 ###\n",
      "Train Loss: 0.6337\n",
      "### ITER 26200 ###\n",
      "Train Loss: 1.1313\n",
      "### ITER 26250 ###\n",
      "Train Loss: 1.2634\n",
      "### ITER 26300 ###\n",
      "Train Loss: 0.8738\n",
      "### ITER 26350 ###\n",
      "Train Loss: 0.6135\n",
      "### ITER 26400 ###\n",
      "Train Loss: 1.1215\n",
      "### ITER 26450 ###\n",
      "Train Loss: 1.3083\n",
      "### ITER 26500 ###\n",
      "Train Loss: 1.3216\n",
      "### ITER 26550 ###\n",
      "Train Loss: 1.2585\n",
      "### ITER 26600 ###\n",
      "Train Loss: 1.1568\n",
      "### ITER 26650 ###\n",
      "Train Loss: 0.8476\n",
      "### ITER 26700 ###\n",
      "Train Loss: 1.0139\n",
      "### ITER 26750 ###\n",
      "Train Loss: 0.5753\n",
      "### ITER 26800 ###\n",
      "Train Loss: 0.7295\n",
      "### ITER 26850 ###\n",
      "Train Loss: 0.4238\n",
      "### ITER 26900 ###\n",
      "Train Loss: 1.0018\n",
      "### ITER 26950 ###\n",
      "Train Loss: 1.5419\n",
      "### ITER 27000 ###\n",
      "Train Loss: 0.6377\n",
      "### ITER 27050 ###\n",
      "Train Loss: 0.8672\n",
      "### ITER 27100 ###\n",
      "Train Loss: 0.8612\n",
      "### ITER 27150 ###\n",
      "Train Loss: 1.2751\n",
      "### ITER 27200 ###\n",
      "Train Loss: 1.2859\n",
      "### ITER 27250 ###\n",
      "Train Loss: 1.0301\n",
      "### ITER 27300 ###\n",
      "Train Loss: 0.8655\n",
      "### ITER 27350 ###\n",
      "Train Loss: 0.4448\n",
      "### ITER 27400 ###\n",
      "Train Loss: 0.5430\n",
      "### ITER 27450 ###\n",
      "Train Loss: 0.7704\n",
      "### ITER 27500 ###\n",
      "Train Loss: 1.0443\n",
      "### ITER 27550 ###\n",
      "Train Loss: 0.6331\n",
      "### ITER 27600 ###\n",
      "Train Loss: 1.1666\n",
      "### ITER 27650 ###\n",
      "Train Loss: 0.6737\n",
      "### ITER 27700 ###\n",
      "Train Loss: 1.7529\n",
      "### ITER 27750 ###\n",
      "Train Loss: 1.2916\n",
      "### ITER 27800 ###\n",
      "Train Loss: 1.5218\n",
      "### ITER 27850 ###\n",
      "Train Loss: 1.1199\n",
      "### ITER 27900 ###\n",
      "Train Loss: 1.1461\n",
      "### ITER 27950 ###\n",
      "Train Loss: 0.6563\n",
      "### ITER 28000 ###\n",
      "Train Loss: 1.1945\n",
      "### ITER 28050 ###\n",
      "Train Loss: 1.4751\n",
      "### ITER 28100 ###\n",
      "Train Loss: 0.9255\n",
      "### ITER 28150 ###\n",
      "Train Loss: 0.7424\n",
      "### ITER 28200 ###\n",
      "Train Loss: 1.1767\n",
      "### ITER 28250 ###\n",
      "Train Loss: 0.4688\n",
      "### ITER 28300 ###\n",
      "Train Loss: 1.3126\n",
      "### ITER 28350 ###\n",
      "Train Loss: 0.9360\n",
      "### ITER 28400 ###\n",
      "Train Loss: 1.0159\n",
      "### ITER 28450 ###\n",
      "Train Loss: 1.6668\n",
      "### ITER 28500 ###\n",
      "Train Loss: 0.8773\n",
      "### ITER 28550 ###\n",
      "Train Loss: 1.1568\n",
      "### ITER 28600 ###\n",
      "Train Loss: 0.9856\n",
      "### ITER 28650 ###\n",
      "Train Loss: 0.5936\n",
      "### ITER 28700 ###\n",
      "Train Loss: 1.4140\n",
      "### ITER 28750 ###\n",
      "Train Loss: 0.8372\n",
      "### ITER 28800 ###\n",
      "Train Loss: 1.7640\n",
      "### ITER 28850 ###\n",
      "Train Loss: 0.6309\n",
      "### ITER 28900 ###\n",
      "Train Loss: 0.7833\n",
      "### ITER 28950 ###\n",
      "Train Loss: 1.4136\n",
      "### ITER 29000 ###\n",
      "Train Loss: 0.5780\n",
      "### ITER 29050 ###\n",
      "Train Loss: 2.1243\n",
      "### ITER 29100 ###\n",
      "Train Loss: 0.8440\n",
      "### ITER 29150 ###\n",
      "Train Loss: 1.2253\n",
      "### ITER 29200 ###\n",
      "Train Loss: 0.6543\n",
      "### ITER 29250 ###\n",
      "Train Loss: 1.4583\n",
      "### ITER 29300 ###\n",
      "Train Loss: 0.9212\n",
      "### ITER 29350 ###\n",
      "Train Loss: 1.3706\n",
      "### ITER 29400 ###\n",
      "Train Loss: 0.5915\n",
      "### ITER 29450 ###\n",
      "Train Loss: 1.4028\n",
      "### ITER 29500 ###\n",
      "Train Loss: 0.9109\n",
      "### ITER 29550 ###\n",
      "Train Loss: 1.4698\n",
      "### ITER 29600 ###\n",
      "Train Loss: 0.8029\n",
      "### ITER 29650 ###\n",
      "Train Loss: 1.2436\n",
      "### ITER 29700 ###\n",
      "Train Loss: 0.9035\n",
      "### ITER 29750 ###\n",
      "Train Loss: 1.0737\n",
      "### ITER 29800 ###\n",
      "Train Loss: 1.3236\n",
      "### ITER 29850 ###\n",
      "Train Loss: 0.4255\n",
      "### ITER 29900 ###\n",
      "Train Loss: 1.0335\n",
      "### ITER 29950 ###\n",
      "Train Loss: 1.8280\n",
      "### ITER 30000 ###\n",
      "Train Loss: 1.4390\n",
      "### ITER 30050 ###\n",
      "Train Loss: 0.6460\n",
      "### ITER 30100 ###\n",
      "Train Loss: 0.7300\n",
      "### ITER 30150 ###\n",
      "Train Loss: 1.1514\n",
      "### ITER 30200 ###\n",
      "Train Loss: 1.4429\n",
      "### ITER 30250 ###\n",
      "Train Loss: 1.2103\n",
      "### ITER 30300 ###\n",
      "Train Loss: 1.0655\n",
      "### ITER 30350 ###\n",
      "Train Loss: 1.0301\n",
      "### ITER 30400 ###\n",
      "Train Loss: 1.0108\n",
      "### ITER 30450 ###\n",
      "Train Loss: 1.2705\n",
      "### ITER 30500 ###\n",
      "Train Loss: 0.8161\n",
      "### ITER 30550 ###\n",
      "Train Loss: 1.0131\n",
      "### ITER 30600 ###\n",
      "Train Loss: 0.8006\n",
      "### ITER 30650 ###\n",
      "Train Loss: 0.7884\n",
      "### ITER 30700 ###\n",
      "Train Loss: 1.4414\n",
      "### ITER 30750 ###\n",
      "Train Loss: 0.7466\n",
      "### ITER 30800 ###\n",
      "Train Loss: 1.4838\n",
      "### ITER 30850 ###\n",
      "Train Loss: 1.2769\n",
      "### ITER 30900 ###\n",
      "Train Loss: 1.3584\n",
      "### ITER 30950 ###\n",
      "Train Loss: 1.1176\n",
      "### ITER 31000 ###\n",
      "Train Loss: 0.4278\n",
      "### ITER 31050 ###\n",
      "Train Loss: 1.0442\n",
      "### ITER 31100 ###\n",
      "Train Loss: 1.6102\n",
      "### ITER 31150 ###\n",
      "Train Loss: 0.3749\n",
      "### ITER 31200 ###\n",
      "Train Loss: 0.8763\n",
      "### ITER 31250 ###\n",
      "Train Loss: 0.4498\n",
      "### ITER 31300 ###\n",
      "Train Loss: 1.1449\n",
      "### ITER 31350 ###\n",
      "Train Loss: 1.3526\n",
      "### ITER 31400 ###\n",
      "Train Loss: 1.3726\n",
      "### ITER 31450 ###\n",
      "Train Loss: 2.0897\n",
      "### ITER 31500 ###\n",
      "Train Loss: 0.9567\n",
      "### ITER 31550 ###\n",
      "Train Loss: 1.1285\n",
      "### ITER 31600 ###\n",
      "Train Loss: 1.2355\n",
      "### ITER 31650 ###\n",
      "Train Loss: 1.0990\n",
      "### ITER 31700 ###\n",
      "Train Loss: 1.2080\n",
      "### ITER 31750 ###\n",
      "Train Loss: 0.6346\n",
      "### ITER 31800 ###\n",
      "Train Loss: 1.4482\n",
      "### ITER 31850 ###\n",
      "Train Loss: 1.1142\n",
      "### ITER 31900 ###\n",
      "Train Loss: 0.9516\n",
      "### ITER 31950 ###\n",
      "Train Loss: 1.2372\n",
      "### ITER 32000 ###\n",
      "Train Loss: 0.7142\n",
      "### ITER 32050 ###\n",
      "Train Loss: 1.2415\n",
      "### ITER 32100 ###\n",
      "Train Loss: 0.6540\n",
      "### ITER 32150 ###\n",
      "Train Loss: 0.4978\n",
      "### ITER 32200 ###\n",
      "Train Loss: 1.2231\n",
      "### ITER 32250 ###\n",
      "Train Loss: 1.0404\n",
      "### ITER 32300 ###\n",
      "Train Loss: 1.1545\n",
      "### ITER 32350 ###\n",
      "Train Loss: 1.1843\n",
      "### ITER 32400 ###\n",
      "Train Loss: 1.6038\n",
      "### ITER 32450 ###\n",
      "Train Loss: 0.8969\n",
      "### ITER 32500 ###\n",
      "Train Loss: 0.7295\n",
      "### ITER 32550 ###\n",
      "Train Loss: 0.6666\n",
      "### ITER 32600 ###\n",
      "Train Loss: 1.2608\n",
      "### ITER 32650 ###\n",
      "Train Loss: 0.7117\n",
      "### ITER 32700 ###\n",
      "Train Loss: 1.5842\n",
      "### ITER 32750 ###\n",
      "Train Loss: 0.9353\n",
      "### ITER 32800 ###\n",
      "Train Loss: 1.2583\n",
      "### ITER 32850 ###\n",
      "Train Loss: 1.6545\n",
      "### ITER 32900 ###\n",
      "Train Loss: 1.7481\n",
      "### ITER 32950 ###\n",
      "Train Loss: 0.4748\n",
      "### ITER 33000 ###\n",
      "Train Loss: 1.1643\n",
      "### ITER 33050 ###\n",
      "Train Loss: 0.4395\n",
      "### ITER 33100 ###\n",
      "Train Loss: 0.8981\n",
      "### ITER 33150 ###\n",
      "Train Loss: 1.7618\n",
      "### ITER 33200 ###\n",
      "Train Loss: 1.1094\n",
      "### ITER 33250 ###\n",
      "Train Loss: 1.4713\n",
      "### ITER 33300 ###\n",
      "Train Loss: 0.9634\n",
      "### ITER 33350 ###\n",
      "Train Loss: 1.3104\n",
      "### ITER 33400 ###\n",
      "Train Loss: 0.7399\n",
      "### ITER 33450 ###\n",
      "Train Loss: 0.7784\n",
      "### ITER 33500 ###\n",
      "Train Loss: 0.5209\n",
      "### ITER 33550 ###\n",
      "Train Loss: 1.5838\n",
      "### ITER 33600 ###\n",
      "Train Loss: 1.2215\n",
      "### ITER 33650 ###\n",
      "Train Loss: 1.0265\n",
      "### ITER 33700 ###\n",
      "Train Loss: 0.8095\n",
      "### ITER 33750 ###\n",
      "Train Loss: 1.0666\n",
      "### ITER 33800 ###\n",
      "Train Loss: 0.7957\n",
      "### ITER 33850 ###\n",
      "Train Loss: 0.9785\n",
      "### ITER 33900 ###\n",
      "Train Loss: 0.7511\n",
      "### ITER 33950 ###\n",
      "Train Loss: 1.3609\n",
      "### ITER 34000 ###\n",
      "Train Loss: 1.2613\n",
      "### ITER 34050 ###\n",
      "Train Loss: 1.4873\n",
      "### ITER 34100 ###\n",
      "Train Loss: 1.2238\n",
      "### ITER 34150 ###\n",
      "Train Loss: 1.2112\n",
      "### ITER 34200 ###\n",
      "Train Loss: 1.0065\n",
      "### ITER 34250 ###\n",
      "Train Loss: 1.0377\n",
      "### ITER 34300 ###\n",
      "Train Loss: 1.7508\n",
      "### ITER 34350 ###\n",
      "Train Loss: 0.8461\n",
      "### ITER 34400 ###\n",
      "Train Loss: 1.3559\n",
      "### ITER 34450 ###\n",
      "Train Loss: 1.2573\n",
      "### ITER 34500 ###\n",
      "Train Loss: 0.7822\n",
      "### ITER 34550 ###\n",
      "Train Loss: 0.6316\n",
      "### ITER 34600 ###\n",
      "Train Loss: 0.8893\n",
      "### ITER 34650 ###\n",
      "Train Loss: 0.8849\n",
      "### ITER 34700 ###\n",
      "Train Loss: 0.8830\n",
      "### ITER 34750 ###\n",
      "Train Loss: 1.2766\n",
      "### ITER 34800 ###\n",
      "Train Loss: 0.9815\n",
      "### ITER 34850 ###\n",
      "Train Loss: 1.4133\n",
      "### ITER 34900 ###\n",
      "Train Loss: 1.2596\n",
      "### ITER 34950 ###\n",
      "Train Loss: 1.3182\n",
      "### ITER 35000 ###\n",
      "Train Loss: 0.4229\n",
      "### ITER 35050 ###\n",
      "Train Loss: 0.7951\n",
      "### ITER 35100 ###\n",
      "Train Loss: 1.3361\n",
      "### ITER 35150 ###\n",
      "Train Loss: 1.0322\n",
      "### ITER 35200 ###\n",
      "Train Loss: 1.1194\n",
      "### ITER 35250 ###\n",
      "Train Loss: 0.5997\n",
      "### ITER 35300 ###\n",
      "Train Loss: 0.5723\n",
      "### ITER 35350 ###\n",
      "Train Loss: 0.9343\n",
      "### ITER 35400 ###\n",
      "Train Loss: 1.2245\n",
      "### ITER 35450 ###\n",
      "Train Loss: 1.3298\n",
      "### ITER 35500 ###\n",
      "Train Loss: 0.9102\n",
      "### ITER 35550 ###\n",
      "Train Loss: 0.6866\n",
      "### ITER 35600 ###\n",
      "Train Loss: 1.2548\n",
      "### ITER 35650 ###\n",
      "Train Loss: 1.7329\n",
      "### ITER 35700 ###\n",
      "Train Loss: 0.9501\n",
      "### ITER 35750 ###\n",
      "Train Loss: 1.8488\n",
      "### ITER 35800 ###\n",
      "Train Loss: 1.2914\n",
      "### ITER 35850 ###\n",
      "Train Loss: 1.3342\n",
      "### ITER 35900 ###\n",
      "Train Loss: 2.0016\n",
      "### ITER 35950 ###\n",
      "Train Loss: 1.0212\n",
      "### ITER 36000 ###\n",
      "Train Loss: 2.1736\n",
      "### ITER 36050 ###\n",
      "Train Loss: 1.4713\n",
      "### ITER 36100 ###\n",
      "Train Loss: 0.8887\n",
      "### ITER 36150 ###\n",
      "Train Loss: 1.3874\n",
      "### ITER 36200 ###\n",
      "Train Loss: 1.0006\n",
      "### ITER 36250 ###\n",
      "Train Loss: 1.0462\n",
      "### ITER 36300 ###\n",
      "Train Loss: 0.9058\n",
      "### ITER 36350 ###\n",
      "Train Loss: 0.8483\n",
      "### ITER 36400 ###\n",
      "Train Loss: 0.4460\n",
      "### ITER 36450 ###\n",
      "Train Loss: 1.2696\n",
      "### ITER 36500 ###\n",
      "Train Loss: 1.4102\n",
      "### ITER 36550 ###\n",
      "Train Loss: 1.1889\n",
      "### ITER 36600 ###\n",
      "Train Loss: 0.9863\n",
      "### ITER 36650 ###\n",
      "Train Loss: 0.6017\n",
      "### ITER 36700 ###\n",
      "Train Loss: 0.6287\n",
      "### ITER 36750 ###\n",
      "Train Loss: 1.7651\n",
      "### ITER 36800 ###\n",
      "Train Loss: 1.4391\n",
      "### ITER 36850 ###\n",
      "Train Loss: 0.7435\n",
      "### ITER 36900 ###\n",
      "Train Loss: 0.6041\n",
      "### ITER 36950 ###\n",
      "Train Loss: 0.3611\n",
      "### ITER 37000 ###\n",
      "Train Loss: 0.3763\n",
      "### ITER 37050 ###\n",
      "Train Loss: 0.8423\n",
      "### ITER 37100 ###\n",
      "Train Loss: 0.8693\n",
      "### ITER 37150 ###\n",
      "Train Loss: 1.1525\n",
      "### ITER 37200 ###\n",
      "Train Loss: 1.0939\n",
      "### ITER 37250 ###\n",
      "Train Loss: 1.0273\n",
      "### ITER 37300 ###\n",
      "Train Loss: 1.1054\n",
      "### ITER 37350 ###\n",
      "Train Loss: 0.9471\n",
      "### ITER 37400 ###\n",
      "Train Loss: 1.1502\n",
      "### ITER 37450 ###\n",
      "Train Loss: 1.3003\n",
      "### ITER 37500 ###\n",
      "Train Loss: 1.2618\n",
      "### ITER 37550 ###\n",
      "Train Loss: 1.5560\n",
      "### ITER 37600 ###\n",
      "Train Loss: 1.0951\n",
      "### ITER 37650 ###\n",
      "Train Loss: 0.7136\n",
      "### ITER 37700 ###\n",
      "Train Loss: 0.7036\n",
      "### ITER 37750 ###\n",
      "Train Loss: 1.4484\n",
      "### ITER 37800 ###\n",
      "Train Loss: 1.0146\n",
      "### ITER 37850 ###\n",
      "Train Loss: 1.1600\n",
      "### ITER 37900 ###\n",
      "Train Loss: 1.1145\n",
      "### ITER 37950 ###\n",
      "Train Loss: 1.0030\n",
      "### ITER 38000 ###\n",
      "Train Loss: 0.8071\n",
      "### ITER 38050 ###\n",
      "Train Loss: 1.3511\n",
      "### ITER 38100 ###\n",
      "Train Loss: 1.2225\n",
      "### ITER 38150 ###\n",
      "Train Loss: 0.6333\n",
      "### ITER 38200 ###\n",
      "Train Loss: 0.6658\n",
      "### ITER 38250 ###\n",
      "Train Loss: 0.5576\n",
      "### ITER 38300 ###\n",
      "Train Loss: 0.8779\n",
      "### ITER 38350 ###\n",
      "Train Loss: 0.9547\n",
      "### ITER 38400 ###\n",
      "Train Loss: 1.4142\n",
      "### ITER 38450 ###\n",
      "Train Loss: 0.7755\n",
      "### ITER 38500 ###\n",
      "Train Loss: 0.2217\n",
      "### ITER 38550 ###\n",
      "Train Loss: 1.1026\n",
      "### ITER 38600 ###\n",
      "Train Loss: 0.9249\n",
      "### ITER 38650 ###\n",
      "Train Loss: 0.8851\n",
      "### ITER 38700 ###\n",
      "Train Loss: 1.2074\n",
      "### ITER 38750 ###\n",
      "Train Loss: 1.1965\n",
      "### ITER 38800 ###\n",
      "Train Loss: 1.4367\n",
      "### ITER 38850 ###\n",
      "Train Loss: 0.5765\n",
      "### ITER 38900 ###\n",
      "Train Loss: 1.3788\n",
      "### ITER 38950 ###\n",
      "Train Loss: 0.5877\n",
      "### ITER 39000 ###\n",
      "Train Loss: 1.2654\n",
      "### ITER 39050 ###\n",
      "Train Loss: 0.7878\n",
      "### ITER 39100 ###\n",
      "Train Loss: 1.0492\n",
      "### ITER 39150 ###\n",
      "Train Loss: 1.8995\n",
      "### ITER 39200 ###\n",
      "Train Loss: 0.7397\n",
      "### ITER 39250 ###\n",
      "Train Loss: 0.6017\n",
      "### ITER 39300 ###\n",
      "Train Loss: 1.1466\n",
      "### ITER 39350 ###\n",
      "Train Loss: 0.8101\n",
      "### ITER 39400 ###\n",
      "Train Loss: 1.1269\n",
      "### ITER 39450 ###\n",
      "Train Loss: 1.0714\n",
      "### ITER 39500 ###\n",
      "Train Loss: 0.8460\n",
      "### ITER 39550 ###\n",
      "Train Loss: 1.0853\n",
      "### ITER 39600 ###\n",
      "Train Loss: 1.0222\n",
      "### ITER 39650 ###\n",
      "Train Loss: 1.6213\n",
      "### ITER 39700 ###\n",
      "Train Loss: 0.7670\n",
      "### ITER 39750 ###\n",
      "Train Loss: 0.9954\n",
      "### ITER 39800 ###\n",
      "Train Loss: 1.6434\n",
      "### ITER 39850 ###\n",
      "Train Loss: 0.3747\n",
      "### ITER 39900 ###\n",
      "Train Loss: 1.2496\n",
      "### ITER 39950 ###\n",
      "Train Loss: 0.7550\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(training_iters))\n",
    "for iter_num in range(training_iters):\n",
    "    \n",
    "    model.train()\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        # Extract a batch of data\n",
    "        batch = next(iter(train_dataloader))\n",
    "        # remove from batch keys that are not needed\n",
    "        train_batch = {k: v for k, v in batch.items() if k in forward_keys}\n",
    "\n",
    "        outputs = model(**train_batch)\n",
    "        # El modelo calcula su loss, pero podriamos acceder a los logits del modelo\n",
    "        # y las labels del batch y calcular nuestra loss propia\n",
    "        # scale the loss to account for gradient accumulation\n",
    "        loss = outputs.loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    progress_bar.update(1)\n",
    "\n",
    "    \"\"\"\n",
    "    if iter_num % eval_interval == 0:\n",
    "        # scale up to undo the division above\n",
    "        # approximating total loss (exact would have been a sum)\n",
    "        train_loss = loss.item() * gradient_accumulation_steps\n",
    "\n",
    "        val_preds = evaluate(model, val_dataloader, tokenizer)\n",
    "        val_correct = sum([1 for p in val_preds if p.y_true == p.y_pred])\n",
    "        val_accuracy = val_correct / len(val_preds)\n",
    "        #train_loss, train_perplexity = evaluate(model, train_dataloader)\n",
    "\n",
    "        print(f\"### ITER {iter_num} ###\")\n",
    "        # print(f\"Train Loss: {train_loss:.4f} - Train Accuracy: {train_accuracy:.4f}\")\n",
    "        # print(f\"Validation Loss: {val_loss:.4f} - Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} - Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        wandb.log({\n",
    "            \"iter\": iter_num,\n",
    "            \"train/loss\": train_loss,\n",
    "            # \"train/accuracy\": train_accuracy,\n",
    "            # \"val/loss\": val_loss,\n",
    "            \"val/accuracy\": val_accuracy,\n",
    "            \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "        })\n",
    "\n",
    "        preds_table = wandb.Table(columns=[\"Case Index\", \"Case Prompt\", \"Generation\", \"Ground Truth\", \"Prediction\"])\n",
    "        for pred in val_preds:    \n",
    "            preds_table.add_data(\n",
    "                pred.case_index,\n",
    "                pred.case_text,\n",
    "                pred.generation,\n",
    "                pred.y_true,\n",
    "                pred.y_pred\n",
    "            )\n",
    "        run.log({f\"test_completions_iter{iter_num}\": preds_table})\n",
    "    \"\"\"\n",
    "    if iter_num % 50 == 0:\n",
    "        train_loss = loss.item() * gradient_accumulation_steps\n",
    "        print(f\"### ITER {iter_num} ###\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        wandb.log({\n",
    "            \"iter\": iter_num,\n",
    "            \"train/loss\": train_loss,\n",
    "            \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "        })\n",
    "        \n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"checkpoints/{run.name}\")\n",
    "# Log the model checkpoint\n",
    "###artifact = wandb.Artifact(\"checkpoint_and_results\", type=\"models\")\n",
    "###artifact.add_dir(f\"checkpoints/{run.name}\")\n",
    "###run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at checkpoints/celestial-resonance-10\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model saved at checkpoints/{run.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finish the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"\"\"\n",
    "You are given a premise and a hypothesis below. If the premise entails the  hypothesis, return 0. If the premise contradicts the hypothesis, return 2.  Otherwise, if the premise does neither, return 1.\n",
    "\n",
    "### Premise: Sorry but that's how it is.\n",
    "\n",
    "### Hypothesis: This is how things are and there are no apologies about it.\n",
    "\n",
    "### Label: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens_with_prompt = model.generate(\n",
    "    tokenizer(texto, return_tensors=\"pt\")[\"input_ids\"],\n",
    "    # max_length=max_input_len + max_output_tokens,\n",
    "    max_new_tokens=max_output_tokens,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    temperature=temperature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given a premise and a hypothesis below. If the premise entails the  hypothesis, return 0. If the premise contradicts the hypothesis, return 2.  Otherwise, if the premise does neither, return 1.\n",
      "\n",
      "### Premise: Sorry but that's how it is.\n",
      "\n",
      "### Hypothesis: This is how things are and there are no apologies about it.\n",
      "\n",
      "### Label: \n",
      "\n",
      "### Label: 0\n",
      "\n",
      "### Label: 1\n",
      "\n",
      "### Label: 2\n",
      "\n",
      "### Label: 1\n",
      "\n",
      "### Label: 2\n",
      "\n",
      "### Label: 1\n",
      "\n",
      "### Label: 1\n",
      "\n",
      "### Label: 2\n",
      "\n",
      "### Label: 1\n",
      "\n",
      "### Label: 2\n",
      "\n",
      "### Label\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(generated_tokens_with_prompt[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch = next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens_with_prompt = model.generate(\n",
    "    val_batch[prompt_key],\n",
    "    # max_length=max_input_len + max_output_tokens,\n",
    "    max_new_tokens=12,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    temperature=temperature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given a premise and a hypothesis below. If the premise entails the  hypothesis, return 0. If the premise contradicts the hypothesis, return 2.  Otherwise, if the premise does neither, return 1.\n",
      "\n",
      "### Premise: yeah yeah well that's neat do you do you look forward to doing it or do you sometimes have to force yourself until you get started\n",
      "\n",
      "### Hypothesis: that's neat, how often do you usually do this?\n",
      "\n",
      "### Label: \n",
      "\n",
      "### Label: 1\n",
      "\n",
      "### Label: 2\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(generated_tokens_with_prompt[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed72c2a8542b4dc28198288464a4e109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maparla/anaconda3/envs/ml/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/maparla/anaconda3/envs/ml/lib/python3.12/site-packages/transformers/generation/utils.py:1659: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "val_preds = evaluate(model, val_dataloader, tokenizer, max_output_tokens=12)\n",
    "val_correct = sum([1 for p in val_preds if p.y_true == p.y_pred])\n",
    "val_accuracy = val_correct / len(val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.857"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
